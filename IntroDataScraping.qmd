---
title: "Introduction to Web Scraping with R"
subtitle: "Chapter 24"
author: "Your Name Here"
format:
  html:
    toc: true
    toc_depth: 2
    number-sections: true
    number-depth: 2
    theme: superhero
execute: 
  warning: false
  cache: true
---

# 1. Scraping Population Data from Wikipedia


Data on the web are often presented in tables. For instance, we can see a list of countries by population in 1900 on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900)


Web pages are written in HTML (Hyper Text Markup Language) which uses **tags** to describe different aspects of document content. For example, a heading in a document is indicated by `<h1>My Title</h1>` whereas a paragraph would be indicated by `<p>A paragraph of content...</p>`. 


In this tutorial, we will learn how to read data from a table on a web page into R. We will need the package `rvest` to get the data from the web page, and the `stringr` package to clean up the data. We may also need  the `clean_names()` function from **janitor** to repair non-syntactic names.


```{r}
library(tidyverse)
library(rvest)
library(janitor)
```


## a) Reading data into R with `rvest`


To get the population data on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900) into R, we use the `read_html` command from the `xml2` package (which is attached when `rvest` is called) to parse the page to obtain an HTML document. 


We then use the `html_nodes` command that extracts all occurrences of the desired tag. We will be interested in scraping data presented in tables, so in the source code, we look for the table tag: `<table> ... </table>`.


Note: some of the `rvest` commands may be slow depending on your Internet connection and the complexity of the web page.


```{r}
popParse <- read_html("https://en.wikipedia.org/wiki/List_of_countries_by_population_in_2000")
str(popParse)
```


The `read_html` command creates an R object, basically a list, that stores information about the web page.


To extract a table from this web page (this may take some time):


```{r}
popNodes <- html_nodes(popParse, "table")
popNodes
```
There are several tables in this document. By inspecting the output of `popNodes`, we make a guess that we want the seventh table. (Trial and error may be required.) We select the seventh table by using double brackets because we want the content of the 7th item from a list (more on R lists soon):


```{r}
pop <- html_table(popNodes, header = TRUE, fill = TRUE)[[3]]
str(pop) # take a look at the structure
```

## b) Cleaning the data frame


We now have a workable data frame that we can clean using the tidyverse techniques we have been learning. Notice that all the variables have been read in as characters but we would typically regard rank and populations as numeric   For `Rank`, that is because the first observation is the world population and it is not assigned a rank, but rather, the character "-". The population column is seen as character because it contains commas and brackets. We need to clean up the data and convert these columns to be numeric.

Use **tidyverse** and **janitor**  commands to create a tidy data set called **popdata** that incorporates the following:
  * Removes the total world population row  
  * Syntactic names are converted to legal names
  * Character variable are converted to numeric where appropriate 
  * Special characters such as bracketed footnotes have been removed to simplify the appearance.
    + The following regular expression is very useful. Can you figure out what it finds and why you might want to replace it with a ""?
    ` "\\[[^]]+\\]" `
  
You should be able to do this in one continuous pipe but you should build up to it gradually by inspecting intermediate results. 

```{r}
df <- tibble(pop) |> 
  slice(-1) |>  
  clean_names() |> 
  rename(population = population2000estimate) |> 
  mutate(
    rank = parse_number(rank), 
    population = parse_number(population),
    country_territory = str_replace_all(country_territory, "\\[[^]]+\\]", "" )
    )
df
```









# A movie box office example


The web site [Box Office Mojo](http://www.boxofficemojo.com) gives statistics on box office earnings of movies. In addition to daily earnings, the web site also maintains lists of yearly and all time record holders.


We will look at the movies in the top 100 in all time movie worldwide grosses in box office receipts. In particular, we will scrape the data from [Box Office Mojo: All Time Box Office](http://www.boxofficemojo.com/alltime/world/?pagenum=1). The dollar amounts are in millions of dollars and the years marked with "^" indicate that the movie had multiple releases.


```{r}
movieParse<- read_html("https://www.boxofficemojo.com/chart/top_lifetime_gross/?area=XWW")

ranks <-
  movieParse |> 
  html_elements(".mojo-truncate") |> 
  html_text2() |> 
  parse_number()
  

titles <- 
  movieParse |> 
  html_elements(".mojo-field-type-title .a-link-normal") |> 
  html_text2()

gross <-
  movieParse |> 
  html_elements(".mojo-field-type-money") |> 
  html_text2() |>
  tail(-1) |>  # not quite a clean column
  parse_number()
  
year <-
  movieParse |> 
  html_elements(".mojo-field-type-year .a-link-normal") |> 
  html_text2() |> 
  parse_number()
```

Putting it all together in one pipe:

```{r}
moviedata <-
tibble(
  ranks =
    movieParse |> 
    html_elements(".mojo-truncate") |> 
    html_text2() |> 
    parse_number(),
  titles = 
    movieParse |> 
    html_elements(".mojo-field-type-title .a-link-normal") |> 
    html_text2(),
  gross =
    movieParse |> 
    html_elements(".mojo-field-type-money") |> 
    html_text2() |>
    tail(-1) |>  # not quite a clean column
    parse_number(),
  year =
    movieParse |> 
    html_elements(".mojo-field-type-year .a-link-normal") |> 
    html_text2() |> 
    parse_number()
)

str(moviedata)
```




#  A Billboard Top 100 example: Using the SelectorGadget

The website [billboard.com](http://www.billboard.com)
keeps track of top songs, albums and artists from the music industry.


One page lists the hot 100 artists. In the source code, here is the listing for my favorite artists, BTS:
`<div class="chart-list-item   piano-content-overlay__gated-item" data-rank="17" data-artist="" data-title="BTS" data-has-content="true">`.

We can use SelectorGadget to verify that the best css selector is ".chart-list-item__title". Use the pipe to access the list of artists and use `trim = TRUE` in `html_text()` to clean up the text extracted from the node. 

```{r}
billboard <- read_html("https://www.billboard.com/charts/artist-100")

artist <- billboard %>% 
  html_nodes(".chart-list-item__title") %>%
  html_text(trim = TRUE)

head(artist)
```

Try another Billboard top 100 list to see if you can adapt this technique to a similar page.


# 4.  On Your Own


* The web site [BikeRaceInfo](http://www.bikeraceinfo.com/tdf/tdfstats.html) has a table with data on past winners of the Tour de France. Create a cleaned-up data frame of this data.

* The web site [NY Times Best Sellers: Hardcover Fiction](http://www.nytimes.com/books/best-sellers/hardcover-fiction) contains a list of best-selling fiction books. Scrape the names of these top books. Use SelectorGadget to obtain the appropriate css selector. 


# 5. Resources

* [HTML Tutorial](www.w3schools.com)